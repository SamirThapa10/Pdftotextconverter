PopularTruth
MichaelSun
May2019
1Introduction
InthisprojectIinvestigatedierentapproachestopredictingthepopularity
ofanewsarticle,givenarticlemetadataandcontent.Apartfromapplying
supervisedlearningonit,Iaminterestedinwhichfeatures,eitherthoseprepared
bythedatasetcreatororadditionalonesIengineer,carrythemostpredictive
power.
Ina2015CS229project,HeRenandQuanYangusedarandomforestto
achieve69%accuracyinclassifyingarticlesaseitherpopular(
>
1400)sharesor
not.Iaimtooutshinethebaselinesetbymypredecessorsbyengineeringnew
featuresontheextractedarticlecontentitself,combiningwhatI'velearnedin
CS229andpreviousNLPtechniquesI'mfamiliarwith.
2CollectingNews
IobtainedthedatasetofarticleurlsandmetadatafromUCI'sOnlineNews
PopularityDataset.TocreatearobustIusedbeautifulsouptoscrape
allthearticlesofitsauthorandcontent.
3TakingintheNews
3.1Sharesdistribution
TheoverallsharesdistributionisaskewedGaussianorapowerlawcurve.
Therewouldn'tbemuchpointinataskifthedistributionwas
justGaussiannoise,butlikeotherformsofmedianewsarticlesproducenet-
workthatspikesno.shares.Admittedly,thispowerlawmaybemore
pronouncedinthemoresensationalistwebsitesoftoday,butnonethelessthis
motivatesthedistinctionbetweenanarticlethattakesordoesn't(hence

1
3.2TimevsShares
Atstthought,itdoesn'tseemlikewecantakethesharenumberverbatim,
aswemayworrya)thearticle'slongevityandb)theoverallincreaseofvisitors
tothenewssitemayskewtheobservations.Itturnsoutafterlookingatthedata
thattheaveragenumberofshares,takeninchronologicalbatches,wasrelatively
constant.Forourpurposes,we'llassumea)andb)eithercanceloutoraren't
toobigofahindrance,treattheageofthearticleasafeature,andproceed
withoutnormalizingthenumberofshares.
4NaiveBayes
Togetafeelhowusefulvocabularieswillbeformytask,Itokenizedall
sentencesofallarticles,andremovedstopwords.Forexperimentationpurposes,
Ionlytakethest1000articles.
Afterconstructingagoodvocabularyset,Ibuiltafrequencymatrixforthe
articles.AtIthoughtonly[nouns,propernouns]isenough(i.e.seeing
"Kardashian"
)
highshares),butaftergettingonly0.60accuracy,decidedto
systematicallyoptimizeagoodpart-of-speech
2
StartingwithanemptyPOSfeatureset,Iappliedforwardsearch,picking
thePOSincombinationwiththeexistingfeaturesetproducesthehighestdelta
score,andappendittothefeaturesetuntilnofeatureimprovesthescore.The
resultis[pluralnouns,past-participleverb,present-participleverb],withascore
of0.69(ononly1000articles!),whichisagoodsign.
(P.S.)ThemostpredictivewordsforNaiveBayesare,inorder,['shootings',
'subscribers','subjects','prosecutors','clues'].
5KernalizedLinearRegression
UsingtheNaiveBayeslog-probabilityforanarticle,Icanfeeditasa(hope-
fullyuseful)featureinlinearregression.
Iappliedthelinearandsigmoidkernelsonsupportvectorregression,quickly
obtaininganaccuracyof0
:
65onasmallsample,whicheasilyputsshametothe
reported0
:
52accuracyofmypredecessorsusingthesamemethod,sothenew
featuredoesindeedmakeat
Afterdoingthis,Irealizedthemosttellingofvocabulariesaren'twords,but
bigrams(DonaldTrump)orn-grams(UnitedStatesofAmerica).Atthatpoint
IthoughtIneededtoexpandthefeatureset(evenifitmaybecomeverysparse)
andwaystoreducedimensionlater.
6WorkingwiththeArticleItself
6.1Title
Webeginwiththemostpromisingpredictor-thetitle(duh)!Word2Vecis
aformofunsupervisedlearningthatmapsasetofwordstocontinuousvectors
inawaywordsthatappearedinsimilarcontexts(sentences,documents,etc.)
havesimilarfeatureshavehigherdotproducts.Bytakinganaverageofthe
wordvectorsinatitle,wecanapproximatea"title"vector.Theresultingtitle
vectorisappendedasynewnumericalfeatures.
(Later,IwonderediftheTF-IDFmatrixitselfofthetitlecanbeuseful,soI
appliedasimilarapproachasworkingwiththecontent(seebelow);Iaddedin
bigramsandtrigramstoformavocabularytogetanincrediblysparseTF-IDF
matrix,butafterdoingLSAthisactuallyworsenedtheaccuracy,soIdecided
toleavethispartout.Youcanseethistangentinthecode).
6.2Keywords
Althoughitwould'vemadesensetoapplythesameapproachasthetitle,I
alsorecognizethatmanyarticleshavekeywords,andIwanttomake
3
surecommononeslike"world"or"society"areweightedlessthanwordslike
"crime",sosimilartothetitle,IappliedaTF-IDFtransformation.
6.3Content
Again,weformavocabularyofalln-gramsuptohex-grams(wecanpush
further,butsixconsecutivewordsseemlikeareasonableforphrases)so
wecanapplyButwait.That'salotoffeatures!
Itturnsout,pastacertainpointinthepast,thecontentisn'tevenrelevant
anymore.Towhenincreasingthevocabularysetbecomescounterproduc-
tive,Iplotmetricsagainstthestartingbatchfromwhichcontent
isused;thispeakturnsouttobearound22000.
Next,toreducethedimension,wehavetwoapproaches-a)useLassoRe-
gressionwithastrongregularizationconstanttodrivetrivialfeaturecots
(words/n-grams)tozero,orb)useLSAtocapturethevarianceofthetf-idf's
oftheexistingvocabularyset.Thereseemstobea(bias-variance)
betweendimensionreductionviathetwoapproaches,soweexperimenttod
thewhatcombinationofeach.
(IchoseLassobecauseitworkswellwithlarge,sparsefeatures,andTruncat-
edSVDbecauseofitsenesswithtf-idfmatricesfordocuments.)
AfteraparamsearchoverC(regconstant),alphathresholdforcoef
elimination),andreduceddimension
d
,Iobtainvaluesof
f
C;;d
g
=
f
6
:
5
;
0
:
4
;
1500
g
,
where
C
producedthehighestF1scoreviaLassoregressiononthetrainingdata,

reduceddimensionbyafactorof5,and
d
captured
>
70%ofthevariance.
With1610numericalfeaturesready(61ofthembeingtheoriginalmeta-
data),it'stimetoputitalltogether.
4
7Results
Method
EvalAcc.
EvalPrec.
TestAcc.
TestPrec.
(1)LassoRegression
0.67
0.63
?
?
(2)SupportVector
0.65
0.65
?
?
(3)RandomForests
0.68
0.64
?
?
(4)NeuralNetworks
0.66
0.61
?
?
Ihaveyettousethetestset,asIwanttotunethisonthevalidationsetabit
furtherafterthecourseends.
1.Thissetahighlytivebaseline.Somewhatsurprisingly,thisoutper-
formedRidgeRegression,implyingmorefeaturescouldhavebeenelimi-
nated.
2.Thishadarecallandspyof0.65too;it'sseemstohave"converged"
toaverystablepoint.Gettingahighprecisionisespeciallyhardwith
articles,anditseemsndingaseparationboundaryoutperformsallother
models.
3.Thiswasthepreviousproject'sbestresult.Ihadissuessurpassingitas
myengineeredfeaturesdon'ttwellwithrandomforests.
4.Parametersweretunedfromtheheuristicthatthemaxfeaturesconsidered
foreachsplitdecreasesprecision,andminsamplesneededforasplitshould
decreaseasvarianceisloweredwithmoretrees.Asitturnsout,random
forestsaren'tveryeindealingwithalltheengineeredcontinuous
features.
8Conclusion
Thegoalofthisprojectwastopredictthebinarypopularitylabelofanar-
ticlefromitsmetadataandcontent.Iengineerednewfeaturesbyextracting
theword2vecofthetitle,theTF-IDF-weightedvectorsofthekeywords,and
theLSA-reducedTF-IDFcountsobtainedafteraLassofeatureselectiononthe
vocabularysetforthecontent.Withcommonsenseandexperimentation,my
engineeredfeaturesare(inaweaksense)optimalconsideringIdidn'tresort
toanymorepowerfulmethods.EachapproachIusedcleanlyoutperformsthe
metadata-onlycounterpartoftheprojectthatinspiredmine,withtheexcep-
tionofrandomforests,whichasdiscussedabovedoesn'tworkwellwithmy
engineeredfeatures.Itseemslikely0
:
69isafundamentallimittothistask's
performance,asa)thesharesdistributioniscontinuousandalotofarticles
fallcloseto1400shares,andb)thereisaninherentdegreeofnoiseandunpre-
dictabilityinarticlereadership(i.e.whathappensinthehour).
5
9Takeaways/FutureWork
Nonetheless,Igottoexperiencetheuncertaintyandexcitementinvolvedin
applyingmachinelearningtoarelevantprobleminindustry.Textisincredibly
richininformation,butpruningoutthenoiserequiredmetothinkcriticallyand
resourcefully,themostrelevantfeaturesIneed(Table2)andreducing
features/dimensionswhennecessary(LassoandLSA).Ialsogotdeeperdomain
insights,liketherelevanceofkeywords,importanceoftitles,andusefulnessof
wordvectorsfromthecontent.Finally,Ilearnedhowtocombineaheteroge-
neoussetoffeaturestogether,accountingforvariableinteractions/dependencies,
andthebias-variancebetweentmodels(i.e.regressionwith
nofeatureengineeringvsrandomforestson\split"ablefeatures).
Ultimately,thegoalistodigdeeperintothearticleextractthebestpredictors
ofpopularity,butIamcurioushowmuchpredictivepowerisintheurland
sentenceofthearticle.Afterwards,Iwanttoinvestigateamorerecurrent
approachbyorderingthefeatures(i.e.url=>keywords=>title=>sentence
=>paragraph=>etc.)andhavingthealgorithmhaltonceithas\enough
information"(thisissimilartohowhumansreadarticles).Viarewards,the
agentlearnstoprioritizewhichfeaturesaremoreimportantandwhichtolook
at
6
